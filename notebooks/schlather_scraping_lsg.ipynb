{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests import get\n",
    "from urllib.parse import urljoin\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self._session = None\n",
    "        self.daten = {}\n",
    "\n",
    "    def _materialien_login(self) -> None:\n",
    "        login_url = (\"https://www.wim.uni-mannheim.de\"\n",
    "                    \"/schlather/teaching/aktuelle-semester/\")\n",
    "        soup = BeautifulSoup(get(login_url).text, \"lxml\")\n",
    "        # Es gibt zwei Loginforms auf der Seite, wir benötigen die untere:\n",
    "        login_form = soup.find(lambda tag: tag.name == \"form\" and tag.fieldset is not None)\n",
    "        # Erstelle session objekt für Login\n",
    "        self._session = requests.session()\n",
    "        # Logindaten\n",
    "        form_data = {\n",
    "            'user': 'stochastik',\n",
    "            'pass': 'schlather',\n",
    "            'submit': 'Anmelden',\n",
    "            'logintype': 'login',\n",
    "            'pid': 0,\n",
    "            'redirect_url': '/schlather/teaching/aktuelle-semester/materialien/',\n",
    "            'tx_felogin_pi1[noredirect]': 0\n",
    "        }\n",
    "        # Login\n",
    "        self._session.post(login_url, data=form_data)\n",
    "\n",
    "    def _scrape_pdfs(self):\n",
    "        base_url = (\"https://www.wim.uni-mannheim.de\"\n",
    "                   \"/schlather/teaching/aktuelle-semester/materialien/\")\n",
    "        soup = BeautifulSoup(self._session.get(base_url).text, \"lxml\")\n",
    "        # Jede Vorlesung ist in einer 'gridelement accordion-item anchor' CSS Klasse:\n",
    "        for vl in soup.find_all(class_=\"gridelement accordion-item anchor\"):\n",
    "            vl_name = vl.a.h4.text.replace(\"\\xad\", \"\")\n",
    "            # Finde alle .pdf Dateien: (alternativ: href=re.compile(r\"pdf$\"))\n",
    "            pdf_files = [ ]\n",
    "            for link in vl.find_all(\"a\", href=lambda h: h is not None and h.endswith(\"pdf\")):\n",
    "                pdf_files.append(urljoin(base_url, link[\"href\"]))\n",
    "            # Füge die Liste der .pdf-Links in das dict ein:\n",
    "            self.daten[vl_name] = pdf_files\n",
    "    \n",
    "    def _download_file(self, url: str, pfad: pathlib.Path, filename: str) -> None:\n",
    "        req = self._session.get(url, stream=True)\n",
    "        if req.status_code == 200:\n",
    "            with open(pfad / filename, \"wb\") as f:\n",
    "                shutil.copyfileobj(req.raw, f)\n",
    "\n",
    "    def download_all_files(self, verbose=False):\n",
    "        self._materialien_login()\n",
    "        self._scrape_pdfs()\n",
    "\n",
    "        # Lade jetzt die Dateien je Vorlesung\n",
    "        for vl_name, links in self.daten.items():\n",
    "            # Erstelle Ordner für VL, falls dieser noch nicht existiert\n",
    "            p = pathlib.Path().cwd() / vl_name.replace(\":\", \"\")\n",
    "            if not p.exists():\n",
    "                p.mkdir()\n",
    "            # Lade Dateien runter\n",
    "            for link in links:\n",
    "                dateiname = link.split(\"/\")[-1]\n",
    "                if verbose:\n",
    "                    print(f\"Lade [{vl_name}]: {dateiname}...\")\n",
    "                self._download_file(link, p, dateiname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade [Aktuar (DAV)]: DAV_Uebersichtsplan_2022.pdf...\n",
      "Lade [Vorstellungsrunden für BSc (Herbst) und MSc (Frühjahr)]: Vorstellung_BA_HS20.pdf...\n",
      "Lade [HWS 21: Fortgeschrittenenkurs R]: vortrag.pdf...\n",
      "Lade [HWS 21: Fortgeschrittenenkurs R]: FortR1.pdf...\n",
      "Lade [HWS 21: Fortgeschrittenenkurs R]: FortShiny.pdf...\n",
      "Lade [HWS 21: Fortgeschrittenenkurs R]: FortR2.pdf...\n",
      "Lade [HWS 21: Fortgeschrittenenkurs R]: FortR3.pdf...\n",
      "Lade [HWS 21: Fortgeschrittenenkurs R]: FortR4.pdf...\n",
      "Lade [HWS 21: Fortgeschrittenenkurs R]: FortR5.pdf...\n",
      "Lade [HWS 21: Fortgeschrittenenkurs R]: FortR6.pdf...\n",
      "Lade [HWS 21: Fortgeschrittenenkurs R]: FortR7.pdf...\n",
      "Lade [HWS 21: Fortgeschrittenenkurs R]: FortR8.pdf...\n"
     ]
    }
   ],
   "source": [
    "s.download_all_files(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
